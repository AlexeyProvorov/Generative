{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhE1UhWVbAEfgMm99H8A82",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexeyProvorov/Generative/blob/master/Attention_Translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Импортируем необходимые библиотеки\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Устанавливаем устройство для вычислений (CPU или GPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# === 1. Подготовка данных ===\n",
        "\n",
        "# Наборы предложений на английском и французском языках\n",
        "english_sentences = [\n",
        "    \"hello how are you\",\n",
        "    \"i am fine thank you\",\n",
        "    \"what is your name\",\n",
        "    \"my name is john\",\n",
        "    \"nice to meet you\"\n",
        "]\n",
        "\n",
        "french_sentences = [\n",
        "    \"bonjour comment ça va\",\n",
        "    \"je vais bien merci\",\n",
        "    \"quel est ton nom\",\n",
        "    \"mon nom est john\",\n",
        "    \"ravi de vous rencontrer\"\n",
        "]\n",
        "\n",
        "# Функция для создания словаря из списка предложений\n",
        "def build_vocab(sentences):\n",
        "    # Начинаем с специальных токенов\n",
        "    vocab = {\"<PAD>\": 0, \"<SOS>\":1, \"<EOS>\":2, \"<UNK>\":3}\n",
        "    index = 4  # Начальный индекс для новых слов\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.split():\n",
        "            if word not in vocab:\n",
        "                vocab[word] = index\n",
        "                index += 1\n",
        "    return vocab\n",
        "\n",
        "# Создаём словари для английского и французского языков\n",
        "english_vocab = build_vocab(english_sentences)\n",
        "french_vocab = build_vocab(french_sentences)\n",
        "\n",
        "# Функция для преобразования предложения в последовательность индексов\n",
        "def sentence_to_indices(sentence, vocab):\n",
        "    # Разбиваем предложение на слова и заменяем их на индексы\n",
        "    indices = [vocab.get(word, vocab[\"<UNK>\"]) for word in sentence.split()]\n",
        "    # Добавляем токен конца предложения\n",
        "    indices.append(vocab[\"<EOS>\"])\n",
        "    return indices\n",
        "\n",
        "# Преобразуем все предложения в последовательности индексов\n",
        "english_sequences = [sentence_to_indices(sentence, english_vocab) for sentence in english_sentences]\n",
        "french_sequences = [sentence_to_indices(sentence, french_vocab) for sentence in french_sentences]\n",
        "\n",
        "# === 2. Создание датасета и загрузчика данных ===\n",
        "\n",
        "# Класс для датасета перевода\n",
        "class TranslationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, input_sequences, target_sequences):\n",
        "        self.input_sequences = input_sequences  # Список входных последовательностей\n",
        "        self.target_sequences = target_sequences  # Список целевых последовательностей\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_sequences)  # Возвращаем количество образцов\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_sequences[idx], self.target_sequences[idx]  # Возвращаем пару (вход, цель)\n",
        "\n",
        "# Функция для объединения батчей и выравнивания последовательностей\n",
        "def collate_fn(batch):\n",
        "    input_seqs, target_seqs = zip(*batch)\n",
        "\n",
        "    # Находим максимальную длину в батче для входных и целевых последовательностей\n",
        "    max_input_len = max(len(seq) for seq in input_seqs)\n",
        "    max_target_len = max(len(seq) for seq in target_seqs)\n",
        "\n",
        "    # Заполняем последовательности токеном <PAD> до максимальной длины\n",
        "    padded_inputs = [seq + [english_vocab[\"<PAD>\"]] * (max_input_len - len(seq)) for seq in input_seqs]\n",
        "    padded_targets = [seq + [french_vocab[\"<PAD>\"]] * (max_target_len - len(seq)) for seq in target_seqs]\n",
        "\n",
        "    # Преобразуем списки в тензоры\n",
        "    return torch.tensor(padded_inputs, dtype=torch.long), torch.tensor(padded_targets, dtype=torch.long)\n",
        "\n",
        "# Создаём экземпляр датасета\n",
        "dataset = TranslationDataset(english_sequences, french_sequences)\n",
        "\n",
        "# Создаём загрузчик данных\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# === 3. Определение модели ===\n",
        "\n",
        "# Параметры модели\n",
        "INPUT_DIM = len(english_vocab)  # Размер словаря входного языка\n",
        "OUTPUT_DIM = len(french_vocab)  # Размер словаря целевого языка\n",
        "ENC_EMB_DIM = 256  # Размерность эмбеддингов кодера\n",
        "DEC_EMB_DIM = 256  # Размерность эмбеддингов декодера\n",
        "HID_DIM = 512      # Размерность скрытого состояния RNN\n",
        "\n",
        "# Кодер\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)  # Слой эмбеддингов\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, batch_first=True)  # RNN (GRU)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: [batch_size, src_len]\n",
        "        embedded = self.embedding(src)  # [batch_size, src_len, emb_dim]\n",
        "        outputs, hidden = self.rnn(embedded)  # outputs: [batch_size, src_len, hid_dim], hidden: [1, batch_size, hid_dim]\n",
        "        return outputs, hidden  # Возвращаем все выходы и последнее скрытое состояние\n",
        "\n",
        "# Механизм внимания\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hid_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        # Линейный слой для вычисления энергии внимания\n",
        "        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n",
        "        # Вектор контекстного веса\n",
        "        self.v = nn.Parameter(torch.rand(hid_dim))\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: [batch_size, hid_dim]\n",
        "        # encoder_outputs: [batch_size, src_len, hid_dim]\n",
        "        src_len = encoder_outputs.shape[1]\n",
        "\n",
        "        # Повторяем скрытое состояние для каждого шага времени\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch_size, src_len, hid_dim]\n",
        "\n",
        "        # Вычисляем энергию внимания\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch_size, src_len, hid_dim]\n",
        "\n",
        "        # Переставляем измерения для умножения\n",
        "        energy = energy.permute(0, 2, 1)  # [batch_size, hid_dim, src_len]\n",
        "\n",
        "        # Повторяем контекстный вектор\n",
        "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # [batch_size, 1, hid_dim]\n",
        "\n",
        "        # Вычисляем веса внимания\n",
        "        attention = torch.bmm(v, energy).squeeze(1)  # [batch_size, src_len]\n",
        "\n",
        "        # Применяем softmax для нормализации\n",
        "        return torch.softmax(attention, dim=1)  # [batch_size, src_len]\n",
        "\n",
        "# Декодер\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, attention):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.output_dim = output_dim  # Размер словаря выходного языка\n",
        "        self.attention = attention    # Механизм внимания\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)  # Слой эмбеддингов\n",
        "        self.rnn = nn.GRU(hid_dim + emb_dim, hid_dim, batch_first=True)  # RNN (GRU)\n",
        "        self.fc_out = nn.Linear(hid_dim * 2 + emb_dim, output_dim)  # Выходной полносвязный слой\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        # input: [batch_size], hidden: [batch_size, hid_dim], encoder_outputs: [batch_size, src_len, hid_dim]\n",
        "\n",
        "        # Добавляем измерение шага времени\n",
        "        input = input.unsqueeze(1)  # [batch_size, 1]\n",
        "\n",
        "        # Эмбеддинг текущего входного слова\n",
        "        embedded = self.embedding(input)  # [batch_size, 1, emb_dim]\n",
        "\n",
        "        # Вычисляем веса внимания\n",
        "        a = self.attention(hidden, encoder_outputs)  # [batch_size, src_len]\n",
        "\n",
        "        # Приводим форму для умножения\n",
        "        a = a.unsqueeze(1)  # [batch_size, 1, src_len]\n",
        "\n",
        "        # Вычисляем контекстный вектор как взвешенную сумму выходов кодера\n",
        "        weighted = torch.bmm(a, encoder_outputs)  # [batch_size, 1, hid_dim]\n",
        "\n",
        "        # Объединяем эмбеддинг и контекстный вектор\n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)  # [batch_size, 1, emb_dim + hid_dim]\n",
        "\n",
        "        # Пропускаем через RNN\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))  # output: [batch_size, 1, hid_dim], hidden: [1, batch_size, hid_dim]\n",
        "\n",
        "        # Убираем измерение слоя\n",
        "        hidden = hidden.squeeze(0)  # [batch_size, hid_dim]\n",
        "        output = output.squeeze(1)  # [batch_size, hid_dim]\n",
        "        embedded = embedded.squeeze(1)  # [batch_size, emb_dim]\n",
        "        weighted = weighted.squeeze(1)  # [batch_size, hid_dim]\n",
        "\n",
        "        # Предсказываем следующее слово\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))  # [batch_size, output_dim]\n",
        "\n",
        "        return prediction, hidden, a.squeeze(1)  # Возвращаем предсказание, новое скрытое состояние и веса внимания\n",
        "\n",
        "# Объединяем кодер и декодер в модель Seq2Seq\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder  # Кодер\n",
        "        self.decoder = decoder  # Декодер\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        # src: [batch_size, src_len], trg: [batch_size, trg_len]\n",
        "        batch_size = src.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # Тензор для хранения предсказаний\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(device)\n",
        "\n",
        "        # Пропускаем входные данные через кодер\n",
        "        encoder_outputs, hidden = self.encoder(src)  # encoder_outputs: [batch_size, src_len, hid_dim], hidden: [1, batch_size, hid_dim]\n",
        "        hidden = hidden.squeeze(0)  # Приводим hidden к форме [batch_size, hid_dim]\n",
        "\n",
        "        # Начинаем с токена <SOS>\n",
        "        input = trg[:, 0]  # [batch_size]\n",
        "\n",
        "        # Список для хранения весов внимания (необязательно)\n",
        "        attentions = []\n",
        "\n",
        "        # Проходим по каждому шагу времени в целевом предложении\n",
        "        for t in range(1, trg_len):\n",
        "            # Пропускаем через декодер\n",
        "            output, hidden, attention = self.decoder(input, hidden, encoder_outputs)\n",
        "            # Сохраняем предсказание\n",
        "            outputs[:, t, :] = output\n",
        "            # Решаем, использовать ли teacher forcing\n",
        "            teacher_force = np.random.random() < teacher_forcing_ratio\n",
        "            # Получаем слово с максимальной вероятностью\n",
        "            top1 = output.argmax(1)\n",
        "            # Определяем следующий вход для декодера\n",
        "            input = trg[:, t] if teacher_force else top1\n",
        "            # Сохраняем веса внимания (необязательно)\n",
        "            attentions.append(attention.cpu().detach().numpy())\n",
        "\n",
        "        return outputs, attentions  # Возвращаем все предсказания и веса внимания\n",
        "\n",
        "# Создаём экземпляры кодера, декодера и модели Seq2Seq\n",
        "attn = Attention(HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, attn)\n",
        "model = Seq2Seq(enc, dec).to(device)\n",
        "\n",
        "# === 4. Обучение модели ===\n",
        "\n",
        "# Определяем функцию потерь и оптимизатор\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=english_vocab[\"<PAD>\"])  # Игнорируем потери на позициях с <PAD>\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Функция для обучения модели на одной эпохе\n",
        "def train(model, dataloader, optimizer, criterion, clip):\n",
        "    model.train()  # Устанавливаем модель в режим обучения\n",
        "    epoch_loss = 0  # Инициализируем суммарную потерю\n",
        "\n",
        "    for src, trg in dataloader:\n",
        "        # Переносим данные на выбранное устройство\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Обнуляем градиенты\n",
        "\n",
        "        # Пропускаем данные через модель\n",
        "        output, _ = model(src, trg)\n",
        "\n",
        "        # Преобразуем выходы и целевые значения для функции потерь\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:].reshape(-1, output_dim)  # Пропускаем первый токен <SOS>\n",
        "        trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "        # Вычисляем потерю\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        # Обратное распространение ошибки\n",
        "        loss.backward()\n",
        "\n",
        "        # Ограничиваем градиенты\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        # Обновляем параметры\n",
        "        optimizer.step()\n",
        "\n",
        "        # Накопливаем потерю\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)  # Возвращаем среднюю потерю за эпоху\n",
        "\n",
        "# Обучаем модель\n",
        "N_EPOCHS = 1000  # Количество эпох\n",
        "CLIP = 1         # Максимальная норма градиента\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    loss = train(model, dataloader, optimizer, criterion, CLIP)\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Эпоха: {epoch + 1}, Потеря: {loss:.4f}')\n",
        "\n",
        "# === 5. Тестирование модели ===\n",
        "\n",
        "# Функция для перевода предложения с использованием обученной модели\n",
        "def translate_sentence(model, sentence, english_vocab, french_vocab, max_len=10):\n",
        "    model.eval()  # Устанавливаем модель в режим оценки\n",
        "\n",
        "    # Преобразуем входное предложение в индексы\n",
        "    tokens = sentence.split()\n",
        "    indices = [english_vocab.get(token, english_vocab[\"<UNK>\"]) for token in tokens]\n",
        "    src_tensor = torch.LongTensor(indices).unsqueeze(0).to(device)  # [1, src_len]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
        "    hidden = hidden.squeeze(0)  # [1, hid_dim]\n",
        "\n",
        "    # Начинаем с токена <SOS>\n",
        "    trg_indices = [french_vocab[\"<SOS>\"]]\n",
        "\n",
        "    # Список для хранения весов внимания\n",
        "    attentions = []\n",
        "\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor([trg_indices[-1]]).to(device)  # [1]\n",
        "        with torch.no_grad():\n",
        "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
        "        pred_token = output.argmax(1).item()  # Получаем индекс с максимальной вероятностью\n",
        "        trg_indices.append(pred_token)  # Добавляем предсказанный токен в результат\n",
        "        attentions.append(attention.cpu().numpy())  # Сохраняем веса внимания\n",
        "\n",
        "        if pred_token == french_vocab[\"<EOS>\"]:\n",
        "            break\n",
        "\n",
        "    # Преобразуем индексы обратно в слова\n",
        "    trg_tokens = [list(french_vocab.keys())[list(french_vocab.values()).index(idx)] for idx in trg_indices[1:]]\n",
        "\n",
        "    return ' '.join(trg_tokens), attentions  # Возвращаем переведённое предложение и веса внимания\n",
        "\n",
        "# Пример перевода\n",
        "sentence = \"hello how are you\"\n",
        "translation, attentions = translate_sentence(model, sentence, english_vocab, french_vocab)\n",
        "print(f\"Входное предложение: {sentence}\")\n",
        "print(f\"Переведённое предложение: {translation}\")\n",
        "\n",
        "# === 6. Визуализация внимания (необязательно) ===\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def display_attention(sentence, translation, attentions):\n",
        "    # Преобразуем веса внимания в массив NumPy\n",
        "    attention = np.array(attentions)\n",
        "    attention = attention[:len(translation.split()), :len(sentence.split())]\n",
        "\n",
        "    # Создаём тепловую карту\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(attention, annot=True, cmap='viridis',\n",
        "                xticklabels=sentence.split(),\n",
        "                yticklabels=translation.split())\n",
        "    plt.xlabel('Входное предложение')\n",
        "    plt.ylabel('Переведённое предложение')\n",
        "    plt.show()\n",
        "\n",
        "# Вызов функции визуализации\n",
        "display_attention(sentence, translation, attentions)\n"
      ],
      "metadata": {
        "id": "fi-VCRfov_om",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "7d6e0c3f-17c8-478a-ac5d-edaddbe5462c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха: 100, Потеря: 0.0001\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-fbdcad24aa27>\u001b[0m in \u001b[0;36m<cell line: 288>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Эпоха: {epoch + 1}, Потеря: {loss:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-fbdcad24aa27>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;31m# Пропускаем данные через модель\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;31m# Преобразуем выходы и целевые значения для функции потерь\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-fbdcad24aa27>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;31m# Пропускаем через декодер\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0;31m# Сохраняем предсказание\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-fbdcad24aa27>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# Пропускаем через RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# output: [batch_size, 1, hid_dim], hidden: [1, batch_size, hid_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Убираем измерение слоя\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1390\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m             result = _VF.gru(\n\u001b[0m\u001b[1;32m   1393\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m                 \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fY309phTv_lG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}