{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtL3f0kxrgPU3A/z1xWHqz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexeyProvorov/Generative/blob/master/Attention_diffusers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Импорт необходимых библиотек ===\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# === Параметры ===\n",
        "\n",
        "BATCH_SIZE = 64      # Размер батча\n",
        "IMAGE_SIZE = 28      # Размер изображений (для MNIST 28x28)\n",
        "CHANNELS = 1         # Количество каналов (для MNIST 1 канал)\n",
        "NUM_EPOCHS = 5       # Количество эпох обучения\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# === Подготовка данных ===\n",
        "\n",
        "# Трансформации для данных\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),                       # Преобразуем изображения в тензоры\n",
        "    transforms.Normalize((0.5,), (0.5,))         # Нормализуем данные к диапазону [-1, 1]\n",
        "])\n",
        "\n",
        "# Загрузка датасета MNIST\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# === Вспомогательные функции для диффузионной модели ===\n",
        "\n",
        "def linear_beta_schedule(timesteps):\n",
        "    \"\"\"\n",
        "    Генерирует линейное расписание beta от beta_start до beta_end.\n",
        "    \"\"\"\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start, beta_end, timesteps)\n",
        "\n",
        "def get_index_from_list(vals, t, x_shape):\n",
        "    \"\"\"\n",
        "    Извлекает значения по индексу t и преобразует их в нужную форму.\n",
        "    \"\"\"\n",
        "    batch_size = t.shape[0]\n",
        "    out = vals.gather(-1, t.cpu())\n",
        "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
        "\n",
        "# === Реализация механизма внимания ===\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        # Определяем ключ, запрос и значение\n",
        "        self.query = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.key = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))  # Параметр для масштабирования выхода\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Прямой проход механизма внимания.\n",
        "        \"\"\"\n",
        "        batch_size, C, width, height = x.size()\n",
        "\n",
        "        # Преобразуем x для вычисления запросов, ключей и значений\n",
        "        proj_query = self.query(x).view(batch_size, -1, width * height)  # [B, C', N]\n",
        "        proj_key = self.key(x).view(batch_size, -1, width * height)      # [B, C', N]\n",
        "        proj_value = self.value(x).view(batch_size, -1, width * height)  # [B, C, N]\n",
        "\n",
        "        # Вычисляем матрицу внимания\n",
        "        energy = torch.bmm(proj_query.permute(0, 2, 1), proj_key)        # [B, N, N]\n",
        "        attention = torch.softmax(energy, dim=-1)                        # [B, N, N]\n",
        "\n",
        "        # Применяем внимание к значениям\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))          # [B, C, N]\n",
        "        out = out.view(batch_size, C, width, height)                     # [B, C, W, H]\n",
        "\n",
        "        # Возвращаем выход с учетом параметра gamma\n",
        "        out = self.gamma * out + x\n",
        "        return out\n",
        "\n",
        "# === Определение модели UNet с вниманием ===\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, timesteps):\n",
        "        super().__init__()\n",
        "\n",
        "        # Количество каналов на различных уровнях\n",
        "        self.down_channels = [64, 128, 256]   # Каналы для downsampling\n",
        "        self.up_channels = [256, 128, 64]     # Каналы для upsampling\n",
        "        self.timesteps = timesteps\n",
        "\n",
        "        # Начальный слой\n",
        "        self.input_conv = nn.Conv2d(CHANNELS, self.down_channels[0], kernel_size=3, padding=1)\n",
        "\n",
        "        # Слои для спускающейся части (Encoder)\n",
        "        self.down_layers = nn.ModuleList()\n",
        "        for i in range(len(self.down_channels) - 1):\n",
        "            self.down_layers.append(nn.Sequential(\n",
        "                nn.Conv2d(self.down_channels[i], self.down_channels[i+1], kernel_size=3, stride=2, padding=1),\n",
        "                nn.ReLU(),\n",
        "                SelfAttention(self.down_channels[i+1])  # Добавляем слой внимания\n",
        "            ))\n",
        "\n",
        "        # Боттлнек\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(self.down_channels[-1], self.down_channels[-1], kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Слои для восходящей части (Decoder)\n",
        "        self.up_layers = nn.ModuleList()\n",
        "        for i in range(len(self.up_channels) - 1):\n",
        "            self.up_layers.append(nn.Sequential(\n",
        "                nn.ConvTranspose2d(self.up_channels[i], self.up_channels[i+1], kernel_size=4, stride=2, padding=1),\n",
        "                nn.ReLU(),\n",
        "                SelfAttention(self.up_channels[i+1])  # Добавляем слой внимания\n",
        "            ))\n",
        "\n",
        "        # Выходной слой\n",
        "        self.output_conv = nn.Conv2d(self.up_channels[-1], CHANNELS, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        \"\"\"\n",
        "        Прямой проход модели UNet с учетом времени t.\n",
        "        \"\"\"\n",
        "        # Встраиваем временной шаг t (здесь можно добавить реализацию встраивания времени)\n",
        "        t_emb = self.time_embedding(t)\n",
        "\n",
        "        # Начальный сверточный слой\n",
        "        x = self.input_conv(x)\n",
        "\n",
        "        # Сохраняем промежуточные слои для skip-connection\n",
        "        skip_connections = []\n",
        "\n",
        "        # Спускающаяся часть (Encoder)\n",
        "        for layer in self.down_layers:\n",
        "            skip_connections.append(x)  # Сохраняем x до понижения разрешения\n",
        "            x = layer(x)\n",
        "\n",
        "        # Боттлнек\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        # Переворачиваем список skip_connections для удобства\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        # Восходящая часть (Decoder)\n",
        "        for i, layer in enumerate(self.up_layers):\n",
        "            x = layer(x)\n",
        "            # Проверяем, чтобы размеры совпадали\n",
        "            if x.shape != skip_connections[i].shape:\n",
        "                x = nn.functional.interpolate(x, size=skip_connections[i].shape[2:])\n",
        "            x = x + skip_connections[i]  # Добавляем соответствующий skip-connection\n",
        "\n",
        "        # Выходной слой\n",
        "        x = self.output_conv(x)\n",
        "        return x\n",
        "\n",
        "    def time_embedding(self, t):\n",
        "        \"\"\"\n",
        "        Встраивание временного шага t.\n",
        "        (Здесь можно добавить реализацию встраивания времени, например, синусоидальное встраивание)\n",
        "        \"\"\"\n",
        "        return t\n",
        "\n",
        "# === Определение диффузионной модели ===\n",
        "\n",
        "class DiffusionModel:\n",
        "    def __init__(self, model, timesteps=1000):\n",
        "        self.model = model\n",
        "        self.timesteps = timesteps\n",
        "        self.betas = linear_beta_schedule(timesteps).to(DEVICE)\n",
        "        self.alphas = 1.0 - self.betas\n",
        "        self.alpha_hat = torch.cumprod(self.alphas, dim=0)\n",
        "\n",
        "    def add_noise(self, x0, t):\n",
        "        \"\"\"\n",
        "        Добавляет шум к изображению x0 на шаге t.\n",
        "        \"\"\"\n",
        "        sqrt_alpha_hat = torch.sqrt(get_index_from_list(self.alpha_hat, t, x0.shape))\n",
        "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - get_index_from_list(self.alpha_hat, t, x0.shape))\n",
        "        eps = torch.randn_like(x0)\n",
        "        xt = sqrt_alpha_hat * x0 + sqrt_one_minus_alpha_hat * eps\n",
        "        return xt, eps\n",
        "\n",
        "    def p_losses(self, x0, t):\n",
        "        \"\"\"\n",
        "        Вычисляет потери модели на заданном шаге t.\n",
        "        \"\"\"\n",
        "        xt, eps = self.add_noise(x0, t)\n",
        "        eps_pred = self.model(xt, t)\n",
        "        return nn.functional.mse_loss(eps_pred, eps)\n",
        "\n",
        "    def sample(self, img_shape):\n",
        "        \"\"\"\n",
        "        Генерирует новое изображение путем обратного процесса диффузии.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            x = torch.randn(img_shape).to(DEVICE)\n",
        "            for t in reversed(range(self.timesteps)):\n",
        "                t_tensor = torch.tensor([t]).to(DEVICE)\n",
        "                eps_pred = self.model(x, t_tensor)\n",
        "                beta_t = self.betas[t]\n",
        "                alpha_t = self.alphas[t]\n",
        "                alpha_hat_t = self.alpha_hat[t]\n",
        "\n",
        "                if t > 0:\n",
        "                    noise = torch.randn_like(x)\n",
        "                else:\n",
        "                    noise = torch.zeros_like(x)\n",
        "\n",
        "                x = (1 / torch.sqrt(alpha_t)) * (x - (beta_t / torch.sqrt(1 - alpha_hat_t)) * eps_pred) + torch.sqrt(beta_t) * noise\n",
        "        return x\n",
        "\n",
        "# === Обучение модели ===\n",
        "\n",
        "# Инициализируем модель и диффузионный процесс\n",
        "timesteps = 1000\n",
        "model = UNet(timesteps).to(DEVICE)\n",
        "diffusion = DiffusionModel(model, timesteps=timesteps)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Цикл обучения\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    for step, (images, _) in enumerate(train_loader):\n",
        "        images = images.to(DEVICE) * 2 - 1  # Приводим изображения к диапазону [-1, 1]\n",
        "\n",
        "        batch_size = images.size(0)\n",
        "        t = torch.randint(0, timesteps, (batch_size,), device=DEVICE).long()  # Случайные временные шаги\n",
        "\n",
        "        # Вычисляем потери\n",
        "        loss = diffusion.p_losses(images, t)\n",
        "\n",
        "        # Обновляем параметры модели\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Печатаем информацию каждые 100 шагов\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Step [{step}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# === Генерация новых изображений ===\n",
        "\n",
        "# Генерируем новые изображения после обучения\n",
        "generated_images = diffusion.sample((BATCH_SIZE, CHANNELS, IMAGE_SIZE, IMAGE_SIZE))\n",
        "\n",
        "# Преобразуем изображения обратно в диапазон [0, 1] для визуализации\n",
        "generated_images = (generated_images + 1) / 2\n",
        "\n",
        "# Отображаем несколько сгенерированных изображений\n",
        "\n",
        "def show_images(images):\n",
        "    grid = torchvision.utils.make_grid(images, nrow=8)\n",
        "    plt.figure(figsize=(15,15))\n",
        "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "show_images(generated_images[:64])\n"
      ],
      "metadata": {
        "id": "fi-VCRfov_om",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "e704ea1d-5d8c-40eb-8ae8-49656a818021"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [0/938], Loss: 1.7074\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-0a700837b04a>\u001b[0m in \u001b[0;36m<cell line: 225>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;31m# Вычисляем потери\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiffusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;31m# Обновляем параметры модели\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-0a700837b04a>\u001b[0m in \u001b[0;36mp_losses\u001b[0;34m(self, x0, t)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m    191\u001b[0m         \u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0meps_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-0a700837b04a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# Восходящая часть (Decoder)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0;31m# Проверяем, чтобы размеры совпадали\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mskip_connections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-0a700837b04a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Вычисляем матрицу внимания\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_key\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# [B, N, N]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menergy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m                        \u001b[0;31m# [B, N, N]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fY309phTv_lG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}